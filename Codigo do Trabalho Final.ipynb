{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDlGG0DtHMu9"
   },
   "source": [
    "# Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27635,
     "status": "ok",
     "timestamp": 1732643178747,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "FPnbws6zHXiD",
    "outputId": "6c82807d-7b0c-4eeb-bfbc-9d1b125af860"
   },
   "outputs": [],
   "source": [
    "#Vincular drive no google colab\n",
    "#drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4131,
     "status": "ok",
     "timestamp": 1732643237542,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "45SspjJmlWxA",
    "outputId": "f4ee8516-a4f6-47e0-ac41-9bdb119b52de"
   },
   "outputs": [],
   "source": [
    "#Instalacao de Todos os pacotes necessários para execução do código\n",
    "#pip install chart_studio numpy pandas matplotlib joblib keras tensorflow scikit-learn plotly seaborn statsmodels scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 3585,
     "status": "ok",
     "timestamp": 1732643258041,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "SUsvKxNBHMvB",
    "outputId": "8bd06a39-04f7-4aa2-8384-941fcf66cc4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, RepeatVector, TimeDistributed, Flatten\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Bidirectional, LSTM\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go  # ou plotly.graph_objs, dependendo da versão\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "# Basic packages\n",
    "import datetime # manipulating date formats\n",
    "import seaborn as sns # for prettier plots\n",
    "\n",
    "\n",
    "# TIME SERIES\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "\n",
    "# settings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Set seeds to make the experiment more reproducible.\n",
    "from tensorflow.random import set_seed\n",
    "from numpy.random import seed\n",
    "set_seed(42)\n",
    "seed(42)\n",
    "\n",
    "droprate = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5fsQ0YkHMvE"
   },
   "source": [
    "# Pré-processamento dos dados\n",
    "\n",
    "* Carregando os dados\n",
    "* Dividindo em treino, teste e validação\n",
    "* Padronizando os dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando Dados e convertendo para Array do Numpy. Configurado apenas para séries univariadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1732643272589,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "nZm0LV0WHMvF"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1732643275715,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "Oi90QVOzHMvF",
    "outputId": "766ff050-7291-4950-a526-22551a2a747d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>hourly_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-26 01:00:00</td>\n",
       "      <td>65738.635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-26 02:00:00</td>\n",
       "      <td>63941.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-26 03:00:00</td>\n",
       "      <td>62740.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-26 04:00:00</td>\n",
       "      <td>62905.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-26 05:00:00</td>\n",
       "      <td>64536.909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index  hourly_demand\n",
       "0  2022-04-26 01:00:00      65738.635\n",
       "1  2022-04-26 02:00:00      63941.485\n",
       "2  2022-04-26 03:00:00      62740.118\n",
       "3  2022-04-26 04:00:00      62905.592\n",
       "4  2022-04-26 05:00:00      64536.909"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     hourly_demand\n",
      "index                             \n",
      "2022-04-26 01:00:00      65738.635\n",
      "2022-04-26 02:00:00      63941.485\n",
      "2022-04-26 03:00:00      62740.118\n",
      "2022-04-26 04:00:00      62905.592\n",
      "2022-04-26 05:00:00      64536.909\n"
     ]
    }
   ],
   "source": [
    "# Converter a coluna \"Data\" para datetime e definir como índice\n",
    "dataframe['index'] = pd.to_datetime(dataframe['index'], errors='coerce')  # Tratar valores inválidos\n",
    "dataframe = dataframe.dropna(subset=['index'])  # Remover valores inválidos\n",
    "dataframe.set_index('index', inplace=True)\n",
    "\n",
    "# Visualizar o dataframe após a transformação\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1732643280030,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "n-FOTlFtPu-Z"
   },
   "outputs": [],
   "source": [
    "data = dataframe.values  # Converte o DataFrame para um array NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzTezs4RHMvH"
   },
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1732643325377,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "StBlBVKGHMvI"
   },
   "outputs": [],
   "source": [
    "# Definindo a proporção para treino, validação e teste\n",
    "train_prop = 0.7\n",
    "valid_prop = 0.15\n",
    "test_prop = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1732643326963,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "bcwb1KRKHMvI",
    "outputId": "70fd24ad-fd7b-4f2d-c92d-bf988a3f6be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "4200\n",
      "900\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "# Tamanho Total de Cada Conjunto\n",
    "total_data_len = len(dataframe)\n",
    "train_data_len = int(np.floor(total_data_len * train_prop))\n",
    "valid_data_len = int(np.floor(total_data_len * valid_prop))\n",
    "test_data_len = int(np.floor(total_data_len * test_prop))\n",
    "print(total_data_len)\n",
    "print(train_data_len)\n",
    "print(valid_data_len)\n",
    "print(test_data_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_S4jyj1lx8p"
   },
   "source": [
    "### Criando Treino, Validacao e Teste sem Padronizacao dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1732644047166,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "Ru3qstfAHMvI",
    "outputId": "dec474c1-c6fb-4c69-9573-f6dc0fa8cf51"
   },
   "outputs": [],
   "source": [
    "# # Criando conjuntos de treinamento\n",
    "# train_data = data[0:train_data_len, :]\n",
    "# X_train, Y_train = [], []\n",
    "# for i in range(60, len(train_data)):\n",
    "#     X_train.append(train_data[i-60:i, 0])\n",
    "#     Y_train.append(train_data[i, 0])\n",
    "\n",
    "# X_train, Y_train = np.array(X_train), np.array(Y_train)\n",
    "# X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# print(len(X_train))\n",
    "# print(len(Y_train))\n",
    "# print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732644047167,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "QwIPmX8dHMvJ",
    "outputId": "c63b0a4f-5838-47ec-f985-b92d18fb0731"
   },
   "outputs": [],
   "source": [
    "# # Criando o conjunto de validação\n",
    "# valid_data = data[train_data_len:train_data_len + valid_data_len, :]\n",
    "# X_valid, Y_valid = [], []\n",
    "\n",
    "# for i in range(60, len(valid_data)):\n",
    "#     X_valid.append(valid_data[i-60:i, 0])\n",
    "#     Y_valid.append(valid_data[i, 0])\n",
    "\n",
    "# X_valid, y_valid = np.array(X_valid), np.array(Y_valid)\n",
    "# X_valid = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))\n",
    "# Y_valid = np.array(Y_valid)\n",
    "\n",
    "# print(len(X_valid))\n",
    "# print(len(Y_valid))\n",
    "# print(len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732644047167,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "Zmn1iOKXHMvJ",
    "outputId": "98c32941-44af-467c-9ba6-050fceaed213"
   },
   "outputs": [],
   "source": [
    "# # Ajustando o conjunto de teste para garantir que o tamanho de X_test e Y_test sejam iguais\n",
    "# test_data = data[train_data_len + valid_data_len - 60:, :]  # Substitua a linha anterior\n",
    "# Y_test = data[train_data_len + valid_data_len:, :]  # Permanece o mesmo\n",
    "\n",
    "# X_test = []\n",
    "# for i in range(60, len(test_data)):\n",
    "#     X_test.append(test_data[i-60:i, 0])\n",
    "\n",
    "# X_test = np.array(X_test)\n",
    "# X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# print(len(X_test))  # Agora deve corresponder ao tamanho de Y_test\n",
    "# print(len(Y_test))\n",
    "# print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1gVsmz4jKcL"
   },
   "source": [
    "### Treino, Teste e Validacao com padronizacao dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1732643406877,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "H-bzu5NrjSzS",
    "outputId": "056ec51a-c058-4db5-c320-54db6e46f643"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando o escalador\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "\n",
    "# Salvar o scaler para uso posterior\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o número de passos de entrada e saída\n",
    "n_steps_in, n_steps_out = 168, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar de Series Temporais para Amostras e Saída\n",
    "def create_supervised_data(data, n_steps_in, n_steps_out):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
    "        X.append(data[i:i + n_steps_in, 0])  # Todas as colunas, exceto a última\n",
    "        Y.append(data[i + n_steps_in:i + n_steps_in + n_steps_out, 0])  # Apenas a última coluna\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão e Padronização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4032, 168)\n",
      "Y_train shape: (4032, 1)\n"
     ]
    }
   ],
   "source": [
    "# Definir proporção de treino\n",
    "train_size = int(len(data) * train_prop)\n",
    "\n",
    "# Dividir o conjunto de treino\n",
    "train_data = data[:train_size]\n",
    "\n",
    "# Padronizar o conjunto de treino\n",
    "train_scaled = scaler.fit_transform(train_data)\n",
    "\n",
    "X_train, Y_train = create_supervised_data(train_scaled, n_steps_in, n_steps_out)\n",
    "\n",
    "# Verificar shapes do conjunto de treino\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_valid shape: (732, 168)\n",
      "Y_valid shape: (732, 1)\n"
     ]
    }
   ],
   "source": [
    "# Definir proporção de validação\n",
    "valid_size = int(len(data) * valid_prop)\n",
    "\n",
    "# Dividir o conjunto de validação\n",
    "valid_data = data[train_size:train_size + valid_size]\n",
    "\n",
    "# Padronizar o conjunto de validação (usando parâmetros do treino)\n",
    "valid_scaled = scaler.transform(valid_data)\n",
    "\n",
    "# Criar dados supervisionados para validação\n",
    "X_valid, Y_valid = create_supervised_data(valid_scaled, n_steps_in, n_steps_out)\n",
    "\n",
    "# Verificar shapes do conjunto de validação\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"Y_valid shape:\", Y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (732, 168)\n",
      "Y_test shape: (732, 1)\n"
     ]
    }
   ],
   "source": [
    "# O restante dos dados será usado para teste\n",
    "test_data = data[train_size + valid_size:]\n",
    "\n",
    "# Padronizar o conjunto de teste (usando parâmetros do treino)\n",
    "test_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Criar dados supervisionados para teste\n",
    "X_test, Y_test = create_supervised_data(test_scaled, n_steps_in, n_steps_out)\n",
    "\n",
    "# Verificar shapes do conjunto de teste\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conferindo Dimensão das Partições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1732643822521,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "ETJcF9klHMvJ",
    "outputId": "ab71e6a6-e532-4baa-a0b7-d57bcecf26b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train length: 4032\n",
      "Y_train length: 4032\n",
      "X_valid length: 732\n",
      "Y_valid length: 732\n",
      "X_test length: 732\n",
      "Y_test length: 732\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train length: {len(X_train)}')\n",
    "print(f'Y_train length: {len(Y_train)}')\n",
    "print(f'X_valid length: {len(X_valid)}')\n",
    "print(f'Y_valid length: {len(Y_valid)}')\n",
    "print(f'X_test length: {len(X_test)}')\n",
    "print(f'Y_test length: {len(Y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiTZ71NpHMvJ"
   },
   "source": [
    "### Ajuste dos dados para os modelos LSTM e CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1732643827431,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "EzHJX2E7HMvK",
    "outputId": "4b6b620d-65ac-4409-9630-8d6bd55b6d38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape (4032, 168, 1)\n",
      "Validation set shape (732, 168, 1)\n",
      "Test set shape (732, 168, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_series = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_valid_series = X_valid.reshape((X_valid.shape[0], X_valid.shape[1], 1))\n",
    "X_test_series = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print('Train set shape', X_train_series.shape)\n",
    "print('Validation set shape', X_valid_series.shape)\n",
    "print('Test set shape', X_test_series.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etbA9Zu3HMvK"
   },
   "source": [
    "# Funções de Métricas Utilizadas e Plot dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732643492627,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "omO0l77UHMvL"
   },
   "outputs": [],
   "source": [
    "# Função para calcular e exibir as métricas de cada modelo\n",
    "def evaluate_model(model, X_test, Y_test, model_name=\"Modelo\"):\n",
    "    # Realiza as previsões no conjunto de teste\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcula as métricas\n",
    "    mae = mean_absolute_error(Y_test, Y_pred)\n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((Y_test - Y_pred) / Y_test)) * 100\n",
    "\n",
    "    # Exibe os resultados\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAPE: {mape:.4f}%\")\n",
    "\n",
    "    return mae, mse, rmse, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732643492627,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "IDWs3fMaHMvL"
   },
   "outputs": [],
   "source": [
    "# Função para exibir os gráficos lado a lado\n",
    "def plot_multiple_predictions(train_valid_data, y_test, y_pred_models, model_names):\n",
    "    # Definindo o tamanho da figura e o layout (2 gráficos por linha)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()  # Achata o array de eixos para indexação direta\n",
    "\n",
    "    for idx, (y_pred, model_name) in enumerate(zip(y_pred_models, model_names)):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        # Plot dos dados de treino e validação\n",
    "        ax.plot(range(len(train_valid_data)), train_valid_data, label=\"Dados de Treinamento e Validação\", color=\"blue\")\n",
    "\n",
    "        # Plot dos dados de teste\n",
    "        ax.plot(range(len(train_valid_data), len(train_valid_data) + len(y_test)), y_test, label=\"Dados de Teste\", color=\"green\")\n",
    "\n",
    "        # Plot das previsões\n",
    "        ax.plot(range(len(train_valid_data), len(train_valid_data) + len(y_test)), y_pred, label=f\"Previsão {model_name}\", color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        # Configurações do gráfico\n",
    "        ax.set_title(f'Ajuste do Modelo com Previsões: {model_name}')\n",
    "        ax.set_xlabel(\"Tempo\")\n",
    "        ax.set_ylabel(\"Valor\")\n",
    "        ax.legend()\n",
    "\n",
    "    # Ajusta o layout e exibe\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732643492627,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "wxcM00tWHMvL"
   },
   "outputs": [],
   "source": [
    "def plot_multiple_loss(history_standard, history_reg, history_drop, history_reg_drop):\n",
    "\n",
    "    # Gráficos de ajuste combinado no conjunto de treinamento e validação e previsão no conjunto de teste\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    # Modelo Simples\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history_standard.history['loss'], label='Treinamento')\n",
    "    plt.plot(history_standard.history['val_loss'], label='Validação')\n",
    "    plt.title('Modelo Simples: Perda durante o Treinamento')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Perda')\n",
    "    plt.legend()\n",
    "\n",
    "    # Modelo com Regularização\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(history_reg.history['loss'], label='Treinamento')\n",
    "    plt.plot(history_reg.history['val_loss'], label='Validação')\n",
    "    plt.title('Modelo com Regularização: Perda durante o Treinamento')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Perda')\n",
    "    plt.legend()\n",
    "\n",
    "    # Modelo com Dropout\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history_drop.history['loss'], label='Treinamento')\n",
    "    plt.plot(history_drop.history['val_loss'], label='Validação')\n",
    "    plt.title('Modelo com Dropout: Perda durante o Treinamento')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Perda')\n",
    "    plt.legend()\n",
    "\n",
    "    # Modelo com Regularização e Dropout\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(history_reg_drop.history['loss'], label='Treinamento')\n",
    "    plt.plot(history_reg_drop.history['val_loss'], label='Validação')\n",
    "    plt.title('Modelo com Regularização e Dropout: Perda durante o Treinamento')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Perda')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732643492627,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "SXo0w5WxHMvM"
   },
   "outputs": [],
   "source": [
    "def plot_predicts(y_pred, y_pred_reg, y_pred_drop, y_pred_reg_drop):\n",
    "\n",
    "    # Gráficos de predição\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    # Modelo Simples\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(Y_test, label='Real', color='blue')\n",
    "    plt.plot(y_pred, label='Predito (Simples)', color='red')\n",
    "    plt.title('Modelo Simples')\n",
    "    plt.xlabel('Tempo')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.legend()\n",
    "\n",
    "    # Modelo com Regularização\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(Y_test, label='Real', color='blue')\n",
    "    plt.plot(y_pred_reg, label='Predito (Reg)', color='red')\n",
    "    plt.title('Modelo com Regularização')\n",
    "    plt.xlabel('Tempo')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.legend()\n",
    "\n",
    "    # Modelo com Dropout\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(Y_test, label='Real', color='blue')\n",
    "    plt.plot(y_pred_drop, label='Predito (Drop)', color='red')\n",
    "    plt.title('Modelo com Dropout')\n",
    "    plt.xlabel('Tempo')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.legend()\n",
    "\n",
    "    # Modelo com Regularização e Dropout\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(Y_test, label='Real', color='blue')\n",
    "    plt.plot(y_pred_reg_drop, label='Predito (Reg + Drop)', color='red')\n",
    "    plt.title('Modelo com Regularização e Dropout')\n",
    "    plt.xlabel('Tempo')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCnaf861HMvM"
   },
   "source": [
    "# MLP for Time Series Forecasting\n",
    "\n",
    "* First we will use a Multilayer Perceptron model or MLP model, here our model will have input features equal to the window size.\n",
    "* The thing with MLP models is that the model don't take the input as sequenced data, so for the model, it is just receiving inputs and don't treat them as sequenced data, that may be a problem since the model won't see the data with the sequence patter that it has.\n",
    "* Input shape **[samples, timesteps]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkCHuxQdHMvM"
   },
   "source": [
    "### Selecao de Hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1732643837190,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "GVsuoU2UHMvM"
   },
   "outputs": [],
   "source": [
    "epochs_mlp = 50\n",
    "batch_mlp = 32\n",
    "lr = 0.001\n",
    "neu_mlp1 = 256 #Quantidade de Neurônios Utilizados na Primeira Camada\n",
    "neu_mlp2 = 64 #Quantidade de Neurônios Utilizados na Segunda Camada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando a Quantidade de Neuronios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de criação do modelo MLP com 2 camadas ocultas\n",
    "def create_mlp_model(neurons_layer_1, neurons_layer_2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons_layer_1, activation='relu', input_dim=X_train.shape[1]))\n",
    "    model.add(Dense(neurons_layer_2, activation='relu'))  # Segunda camada\n",
    "    model.add(Dense(1))  # Camada de saída para regressão\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Listas de neurônios para a primeira e segunda camada\n",
    "neurons_layer_1_options_mlp = [16, 32, 64, 128, 256]\n",
    "neurons_layer_2_options_mlp = [16, 32, 64, 128, 256]\n",
    "\n",
    "# Armazenar resultados\n",
    "results_mlp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop para testar todas as combinações possíveis de neurônios\n",
    "for neurons_layer_1 in neurons_layer_1_options_mlp:\n",
    "    for neurons_layer_2 in neurons_layer_2_options_mlp:\n",
    "        print(f\"Treinando modelo com {neurons_layer_1} neurônios na primeira camada e {neurons_layer_2} na segunda camada...\")\n",
    "\n",
    "        # Criar o modelo\n",
    "        model = create_mlp_model(neurons_layer_1, neurons_layer_2)\n",
    "\n",
    "        # Treinar o modelo\n",
    "        history = model.fit(X_train, Y_train, epochs=epochs_mlp, batch_size=batch_mlp, validation_data=(X_valid, Y_valid), verbose=0)\n",
    "\n",
    "        # Obter o desempenho (pode ser MAPE, RMSE, etc.)\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "        # Armazenar o desempenho\n",
    "        results_mlp.append({'neurons_layer_1': neurons_layer_1, 'neurons_layer_2': neurons_layer_2, 'val_loss': val_loss})\n",
    "\n",
    "# Converter para dataframe (opcional)\n",
    "import pandas as pd\n",
    "results_df_mlp = pd.DataFrame(results_mlp)\n",
    "\n",
    "# Gerar gráfico\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Gerar gráfico de calor (heatmap)\n",
    "pivot_table_mlp = results_df_mlp.pivot(index='neurons_layer_1', columns='neurons_layer_2', values='val_loss')\n",
    "cax = ax.matshow(pivot_table_mlp, cmap='viridis')\n",
    "\n",
    "# Adicionar rótulos\n",
    "ax.set_xticks(np.arange(len(neurons_layer_2_options_mlp)))\n",
    "ax.set_yticks(np.arange(len(neurons_layer_1_options_mlp)))\n",
    "ax.set_xticklabels(neurons_layer_2_options_mlp)\n",
    "ax.set_yticklabels(neurons_layer_1_options_mlp)\n",
    "\n",
    "# Adicionar título e legendas\n",
    "plt.xlabel('Neurônios na Segunda Camada')\n",
    "plt.ylabel('Neurônios na Primeira Camada')\n",
    "plt.title('Desempenho por Número de Neurônios nas Camadas Ocultas')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTLjxJhXHMvM"
   },
   "source": [
    "### Implementacao e ajuste do Modelo MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1732643842566,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "035G4QL2HMvN"
   },
   "outputs": [],
   "source": [
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(neu_mlp1, activation='relu', input_dim=X_train.shape[1]))\n",
    "model_mlp.add(Dense(neu_mlp2))\n",
    "model_mlp.add(Dense(1))\n",
    "model_mlp.compile(loss='mean_squared_error', optimizer=optimizers.Adam(lr))\n",
    "# model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7926,
     "status": "ok",
     "timestamp": 1732643851849,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "GlkTI8H4HMvN",
    "outputId": "9abc51f5-2431-4a02-9fce-8d44898de8ba"
   },
   "outputs": [],
   "source": [
    "history_mlp = model_mlp.fit(X_train,\n",
    "                            Y_train,\n",
    "                            validation_data=(X_valid, Y_valid),\n",
    "                            epochs=epochs_mlp,\n",
    "                            batch_size=batch_mlp,\n",
    "                            verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTObbUdoHMvN"
   },
   "source": [
    "### Implementacao e Ajuste do Modelo MLP com regularização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1732643853799,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "3Nn3aPPTHMvO"
   },
   "outputs": [],
   "source": [
    "model_mlp_reg = Sequential()\n",
    "model_mlp_reg.add(Dense(neu_mlp1, activation='relu', input_dim=X_train.shape[1],\n",
    "                     kernel_regularizer=regularizers.l2(0.01)))  # Regularização L2\n",
    "model_mlp_reg.add(Dense(neu_mlp2, activation='relu',\n",
    "                     kernel_regularizer=regularizers.l2(0.01)))  # Regularização L2\n",
    "model_mlp_reg.add(Dense(1))  # Saída\n",
    "model_mlp_reg.compile(loss='mean_squared_error', optimizer=optimizers.Adam(lr))\n",
    "# model_mlp_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6864,
     "status": "ok",
     "timestamp": 1732643860658,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "snIfZ-2vHMvO",
    "outputId": "ff440983-51af-4a0d-8b25-52151c08df16"
   },
   "outputs": [],
   "source": [
    "history_mlp_reg = model_mlp_reg.fit(X_train,\n",
    "                                    Y_train,\n",
    "                                    validation_data=(X_valid, Y_valid),\n",
    "                                    epochs=epochs_mlp,\n",
    "                                    batch_size=batch_mlp,\n",
    "                                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTf6e6AeHMvO"
   },
   "source": [
    "### Implementacao e ajuste do modelo MLP com Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1732643862800,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "Mrxcuzq-HMvO"
   },
   "outputs": [],
   "source": [
    "model_mlp_drop = Sequential()\n",
    "model_mlp_drop.add(Dense(neu_mlp1, activation='relu', input_dim=X_train.shape[1]))\n",
    "model_mlp_drop.add(Dropout(droprate))\n",
    "model_mlp_drop.add(Dense(neu_mlp2, activation='relu'))\n",
    "model_mlp_drop.add(Dropout(droprate))\n",
    "model_mlp_drop.add(Dense(1))\n",
    "model_mlp_drop.compile(loss='mean_squared_error', optimizer=optimizers.Adam(lr))\n",
    "# model_mlp_dropout.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7751,
     "status": "ok",
     "timestamp": 1732643870548,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "tPcnnmA1HMvT",
    "outputId": "7ec348cb-746b-42df-e5ee-47288de11f7d"
   },
   "outputs": [],
   "source": [
    "#history_mlp_drop = model_mlp_dropout.fit(X_train, Y_train,\n",
    "#                            validation_data=(X_valid, Y_valid),\n",
    "#                            epochs=epochs_mlp,\n",
    "#                            verbose=2)\n",
    "\n",
    "history_mlp_drop = model_mlp_drop.fit(X_train,\n",
    "                                        Y_train,\n",
    "                                        validation_data=(X_valid, Y_valid),\n",
    "                                        epochs=epochs_mlp,\n",
    "                                        batch_size=batch_mlp,\n",
    "                                        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrkM0NsgHMvU"
   },
   "source": [
    "### Implementando e Ajustando modelo MLP com Regularização e Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1732643872645,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "Z0RyVEQ6HMvU"
   },
   "outputs": [],
   "source": [
    "# Definição do modelo MLP com regularização e dropout\n",
    "model_mlp_reg_drop = Sequential()\n",
    "model_mlp_reg_drop.add(Dense(neu_mlp1, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.001)))\n",
    "model_mlp_reg_drop.add(Dropout(droprate)) \n",
    "model_mlp_reg_drop.add(Dense(neu_mlp2, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model_mlp_reg_drop.add(Dropout(droprate))\n",
    "model_mlp_reg_drop.add(Dense(1))  # Camada de saída\n",
    "model_mlp_reg_drop.compile(optimizer=optimizers.Adam(lr), loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7565,
     "status": "ok",
     "timestamp": 1732643880207,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "t-gOdDmdHMvU",
    "outputId": "a87779f2-d1da-45db-8c90-00084510c444"
   },
   "outputs": [],
   "source": [
    "history_mlp_reg_drop = model_mlp_reg_drop.fit(X_train,\n",
    "                                              Y_train,\n",
    "                                              validation_data=(X_valid, Y_valid),\n",
    "                                              epochs=epochs_mlp,\n",
    "                                              batch_size=batch_mlp,\n",
    "                                              verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y-QoL-uHMvV"
   },
   "source": [
    "## Comparando os 4 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1294,
     "status": "ok",
     "timestamp": 1732643883526,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "VIgGQz5FHMvV",
    "outputId": "3db94434-65ed-4605-a7c1-b262e8eb4832"
   },
   "outputs": [],
   "source": [
    "# Avaliação de cada modelo no conjunto de teste\n",
    "mae_standard, mse_standard, rmse_standard, mape_standard = evaluate_model(model_mlp, X_test, Y_test, \"Modelo Padrão\")\n",
    "mae_reg, mse_reg, rmse_reg, mape_reg = evaluate_model(model_mlp_reg, X_test, Y_test, \"Modelo com Regularização\")\n",
    "mae_dropout, mse_dropout, rmse_dropout, mape_dropout = evaluate_model(model_mlp_drop, X_test, Y_test, \"Modelo com Dropout\")\n",
    "mae_reg_drop, mse_reg_drop, rmse_reg_drop, mape_reg_drop = evaluate_model(model_mlp_reg_drop, X_test, Y_test, \"Modelo com Regularização e Dropout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "executionInfo": {
     "elapsed": 2095,
     "status": "ok",
     "timestamp": 1732643885620,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "rGhR3xCCHMvV",
    "outputId": "b916c073-b1e9-4608-e377-f3fe8d184098"
   },
   "outputs": [],
   "source": [
    "#Plot do Grafico de Perda\n",
    "plot_multiple_loss(history_mlp, history_mlp_reg, history_mlp_drop, history_mlp_reg_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1732643885621,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "5-j77D9CHMvV",
    "outputId": "27e0dfcd-09d1-4c43-bb48-a932b6cc8bd8"
   },
   "outputs": [],
   "source": [
    "# Gerando previsões para cada modelo\n",
    "y_pred_mlp = model_mlp.predict(X_test)\n",
    "y_pred_mlp_reg = model_mlp_reg.predict(X_test)\n",
    "y_pred_mlp_drop = model_mlp_drop.predict(X_test)\n",
    "y_pred_mlp_reg_drop = model_mlp_reg_drop.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688
    },
    "executionInfo": {
     "elapsed": 1259,
     "status": "ok",
     "timestamp": 1732643886874,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "2TQeAg9bHMvW",
    "outputId": "15057334-dbdd-4e8a-ca98-0bb62ed993f8"
   },
   "outputs": [],
   "source": [
    "plot_predicts(y_pred_mlp, y_pred_mlp_reg, y_pred_mlp_drop, y_pred_mlp_reg_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688
    },
    "executionInfo": {
     "elapsed": 834,
     "status": "ok",
     "timestamp": 1732643887700,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "HCgWCejDHMvW",
    "outputId": "91b009b9-420b-44fa-c917-46d7d78b65fa"
   },
   "outputs": [],
   "source": [
    "# Combine os dados de treino e validação em uma série para comparação visual com o conjunto de teste e previsões.\n",
    "train_valid_data = np.concatenate([Y_train, Y_valid])  # Combina Y_train e Y_valid em uma série contínua\n",
    "\n",
    "# Lista com previsões e nomes dos modelos para plotar\n",
    "y_pred_models = [y_pred_mlp, y_pred_mlp_reg, y_pred_mlp_drop, y_pred_mlp_reg_drop]\n",
    "model_names = [\"MLP Padrão\", \"MLP com Regularização\", \"MLP com Dropout\", \"MLP com Regularização e Dropout\"]\n",
    "\n",
    "# Chamando a função para exibir os gráficos lado a lado\n",
    "plot_multiple_predictions(train_valid_data, Y_test, y_pred_models, model_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6wfWEQlHMvX"
   },
   "source": [
    "# CNN for Time Series Forecasting\n",
    "\n",
    "* For the CNN model we will use one convolutional hidden layer followed by a max pooling layer. The filter maps are then flattened before being interpreted by a Dense layer and outputting a prediction.\n",
    "* The convolutional layer should be able to identify patterns between the timesteps.\n",
    "* Input shape **[samples, timesteps, features]**.\n",
    "\n",
    "#### Data preprocess\n",
    "* Reshape from [samples, timesteps] into [samples, timesteps, features].\n",
    "* This same reshaped data will be used on the CNN and the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xWsnz8XHMvX"
   },
   "source": [
    "## Seleção de Hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1732643946561,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "V1qw-Rs9HMvX"
   },
   "outputs": [],
   "source": [
    "#Hiperparametros\n",
    "epochs_cnn = 30\n",
    "batch_cnn = 32\n",
    "lr_cnn = 0.001\n",
    "neu_cnn1 = 32 # Quantidade De Neuronios da Primeira camada\n",
    "neu_cnn2 = 32 #Quantidade de Neuronios da Segunda Camada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando a Quantidade de Neuronios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de criação do modelo MLP com 2 camadas ocultas\n",
    "def create_mlp_model(neurons_layer_1, neurons_layer_2):\n",
    "    model_cnn = Sequential()\n",
    "    model_cnn.add(Conv1D(filters=neurons_layer_1, kernel_size=2, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "    model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "    model_cnn.add(Flatten())\n",
    "    model_cnn.add(Dense(neurons_layer_2, activation='relu'))\n",
    "    model_cnn.add(Dense(1))\n",
    "    model_cnn.compile(loss='mse', optimizer=optimizers.Adam(lr_cnn))\n",
    "    return model\n",
    "\n",
    "# Listas de neurônios para a primeira e segunda camada\n",
    "neurons_layer_1_options_cnn = [32, 64, 128, 256]\n",
    "neurons_layer_2_options_cnn = [32, 64, 128, 256]\n",
    "\n",
    "# Armazenar resultados\n",
    "results_cnn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop para testar todas as combinações possíveis de neurônios\n",
    "for neurons_layer_1 in neurons_layer_1_options_cnn:\n",
    "    for neurons_layer_2 in neurons_layer_2_options_cnn:\n",
    "        print(f\"Treinando modelo com {neurons_layer_1} neurônios na primeira camada e {neurons_layer_2} na segunda camada...\")\n",
    "\n",
    "        # Criar o modelo\n",
    "        model = create_mlp_model(neurons_layer_1, neurons_layer_2)\n",
    "\n",
    "        # Treinar o modelo\n",
    "        history = model.fit(X_train, Y_train, epochs=35, batch_size=32, validation_data=(X_valid, Y_valid), verbose=0)\n",
    "\n",
    "        # Obter o desempenho (pode ser MAPE, RMSE, etc.)\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "        # Armazenar o desempenho\n",
    "        results_cnn.append({'neurons_layer_1': neurons_layer_1, 'neurons_layer_2': neurons_layer_2, 'val_loss': val_loss})\n",
    "\n",
    "\n",
    "results_df_cnn = pd.DataFrame(results_cnn)\n",
    "\n",
    "# Gerar gráfico\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Gerar gráfico de calor (heatmap)\n",
    "pivot_table = results_df_cnn.pivot(index='neurons_layer_1', columns='neurons_layer_2', values='val_loss')\n",
    "cax = ax.matshow(pivot_table, cmap='viridis')\n",
    "\n",
    "# Adicionar rótulos\n",
    "ax.set_xticks(np.arange(len(neurons_layer_2_options_cnn)))\n",
    "ax.set_yticks(np.arange(len(neurons_layer_1_options_cnn)))\n",
    "ax.set_xticklabels(neurons_layer_2_options_cnn)\n",
    "ax.set_yticklabels(neurons_layer_1_options_cnn)\n",
    "\n",
    "# Adicionar título e legendas\n",
    "plt.xlabel('Neurônios na Segunda Camada')\n",
    "plt.ylabel('Neurônios na Primeira Camada')\n",
    "plt.title('Desempenho por Número de Neurônios nas Camadas Ocultas')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0Y-S_QoHMvX"
   },
   "source": [
    "## Implementando e Ajustando Modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1732643950030,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "O_IahkzfHMvY"
   },
   "outputs": [],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(filters=neu_cnn1, kernel_size=2, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(neu_cnn2, activation='relu'))\n",
    "model_cnn.add(Dense(1))\n",
    "model_cnn.compile(loss='mse', optimizer=optimizers.Adam(lr_cnn))\n",
    "# model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5518,
     "status": "ok",
     "timestamp": 1732643955545,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "2FPO52o2HMvY",
    "outputId": "e536408d-07e2-4918-844a-c2a987a7253c"
   },
   "outputs": [],
   "source": [
    "history_cnn = model_cnn.fit(X_train,\n",
    "                            Y_train,\n",
    "                            validation_data=(X_valid, Y_valid),\n",
    "                            epochs=epochs_cnn,\n",
    "                            batch_size=batch_cnn,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMyajQqEHMvY"
   },
   "source": [
    "## Implementando e Ajustando Modelo CNN com Regularização L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732643955546,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "THCX7HKWHMvZ"
   },
   "outputs": [],
   "source": [
    "model_cnn_reg = Sequential()\n",
    "model_cnn_reg.add(Conv1D(filters=neu_cnn1, kernel_size=2, activation='relu',\n",
    "                         kernel_regularizer=regularizers.l2(0.01),  # Regularização L2\n",
    "                         input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_cnn_reg.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn_reg.add(Flatten())\n",
    "model_cnn_reg.add(Dense(neu_cnn2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model_cnn_reg.add(Dense(1))\n",
    "model_cnn_reg.compile(loss='mse', optimizer=optimizers.Adam(lr_cnn))\n",
    "# model_cnn_reg.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7632,
     "status": "ok",
     "timestamp": 1732643963174,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "TnhbZ4-SHMvZ",
    "outputId": "c43614dc-0bd7-469f-ce45-14a8f175df56"
   },
   "outputs": [],
   "source": [
    "#history_cnn_reg = model_cnn_reg.fit(X_train_series, Y_train,\n",
    "#                            validation_data=(X_valid_series, Y_valid),\n",
    "#                            epochs=epochs_cnn,\n",
    "#                            verbose=2)\n",
    "\n",
    "history_cnn_reg = model_cnn_reg.fit(X_train_series,\n",
    "                                    Y_train,\n",
    "                                    validation_data=(X_valid_series, Y_valid),\n",
    "                                    epochs=epochs_cnn,\n",
    "                                    batch_size=batch_cnn,\n",
    "                                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peMSNFacHMvZ"
   },
   "source": [
    "## Implementando e Ajustando Modelo CNN com Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732643963174,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "_BRwmh2jHMvZ"
   },
   "outputs": [],
   "source": [
    "model_cnn_drop = Sequential()\n",
    "model_cnn_drop.add(Conv1D(filters=neu_cnn1, kernel_size=2, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_cnn_drop.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn_drop.add(Dropout(droprate))\n",
    "model_cnn_drop.add(Flatten())\n",
    "model_cnn_drop.add(Dense(neu_cnn2, activation='relu'))\n",
    "model_cnn_drop.add(Dropout(droprate))\n",
    "model_cnn_drop.add(Dense(1))\n",
    "model_cnn_drop.compile(loss='mse', optimizer=optimizers.Adam(lr_cnn))\n",
    "# model_cnn_drop.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5419,
     "status": "ok",
     "timestamp": 1732643968589,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "e2ahr5NMHMvZ",
    "outputId": "0e8c77fd-a35c-428d-97ec-d156f21d96fa"
   },
   "outputs": [],
   "source": [
    "history_cnn_drop = model_cnn_drop.fit(X_train_series,\n",
    "                                    Y_train,\n",
    "                                    validation_data=(X_valid_series, Y_valid),\n",
    "                                    epochs=epochs_cnn,\n",
    "                                    batch_size=batch_cnn,\n",
    "                                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-MOxkpfHMva"
   },
   "source": [
    "## Implementando e Ajustando Modelo CNN com Regularização e Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732643968590,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "nJkyaItBHMva"
   },
   "outputs": [],
   "source": [
    "model_cnn_reg_drop = Sequential()\n",
    "model_cnn_reg_drop.add(Conv1D(filters=neu_cnn1, kernel_size=2, activation='relu',\n",
    "                             kernel_regularizer=regularizers.l2(0.01),\n",
    "                             input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_cnn_reg_drop.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn_reg_drop.add(Dropout(droprate))\n",
    "model_cnn_reg_drop.add(Flatten())\n",
    "model_cnn_reg_drop.add(Dense(neu_cnn2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model_cnn_reg_drop.add(Dropout(droprate))\n",
    "model_cnn_reg_drop.add(Dense(1))\n",
    "model_cnn_reg_drop.compile(loss='mse', optimizer=optimizers.Adam(lr_cnn))\n",
    "# model_cnn_reg_drop.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7936,
     "status": "ok",
     "timestamp": 1732643976520,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "GkoSSQeoHMva",
    "outputId": "8f0c8bde-369a-4c14-ade0-da7d9397ebea"
   },
   "outputs": [],
   "source": [
    "history_cnn_reg_drop = model_cnn_reg_drop.fit(X_train_series,\n",
    "                                            Y_train,\n",
    "                                            validation_data=(X_valid_series, Y_valid),\n",
    "                                            epochs=epochs_cnn,\n",
    "                                            batch_size=batch_cnn,\n",
    "                                            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5p0Pek-HMva"
   },
   "source": [
    "## Comparando os 4 Modelos CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 847,
     "status": "ok",
     "timestamp": 1732643977365,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "npgmpMQIHMva",
    "outputId": "e839dc6b-2791-4e70-919d-5012526dd4c0"
   },
   "outputs": [],
   "source": [
    "# Previsões\n",
    "y_pred_cnn = model_cnn.predict(X_test)\n",
    "y_pred_cnn_reg = model_cnn_reg.predict(X_test)\n",
    "y_pred_cnn_drop = model_cnn_drop.predict(X_test)\n",
    "y_pred_cnn_reg_drop = model_cnn_reg_drop.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1732643977744,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "W5e7PHUlHMvb",
    "outputId": "e0065155-bf62-4515-a878-9c3e2374a3d7"
   },
   "outputs": [],
   "source": [
    "# Avaliação de cada modelo no conjunto de teste\n",
    "mae_standard, mse_standard, rmse_standard, mape_standard = evaluate_model(model_cnn, X_test, Y_test, \"Modelo Padrão\")\n",
    "mae_reg, mse_reg, rmse_reg, mape_reg = evaluate_model(model_cnn_reg, X_test, Y_test, \"Modelo com Regularização\")\n",
    "mae_dropout, mse_dropout, rmse_dropout, mape_dropout = evaluate_model(model_cnn_drop, X_test, Y_test, \"Modelo com Dropout\")\n",
    "mae_reg_drop, mse_reg_drop, rmse_reg_drop, mape_reg_drop = evaluate_model(model_cnn_reg_drop, X_test, Y_test, \"Modelo com Regularização e Dropout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "executionInfo": {
     "elapsed": 1554,
     "status": "ok",
     "timestamp": 1732643979294,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "6zqiAnGJHMvb",
    "outputId": "5a017696-23c4-4092-c736-76a0fa9de05e"
   },
   "outputs": [],
   "source": [
    "#Plot do Grafico de Perda\n",
    "plot_multiple_loss(history_cnn, history_cnn_reg, history_cnn_drop, history_cnn_reg_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688
    },
    "executionInfo": {
     "elapsed": 985,
     "status": "ok",
     "timestamp": 1732643980274,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "2ZkNB6cYHMvb",
    "outputId": "de8201e0-3f52-4635-ff4f-46b395082198"
   },
   "outputs": [],
   "source": [
    "plot_predicts(y_pred_cnn, y_pred_cnn_reg, y_pred_cnn_drop, y_pred_cnn_reg_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688
    },
    "executionInfo": {
     "elapsed": 1159,
     "status": "ok",
     "timestamp": 1732643981428,
     "user": {
      "displayName": "Lucas Camaz Ferreira",
      "userId": "04538435179332066433"
     },
     "user_tz": 180
    },
    "id": "aat9w9e5HMvb",
    "outputId": "90232cf5-aa02-4221-d66a-bb555b53cd6a"
   },
   "outputs": [],
   "source": [
    "# Combine os dados de treino e validação em uma série para comparação visual com o conjunto de teste e previsões.\n",
    "train_valid_data = np.concatenate([Y_train, Y_valid])  # Combina Y_train e Y_valid em uma série contínua\n",
    "\n",
    "# Lista com previsões e nomes dos modelos para plotar\n",
    "y_pred_models = [y_pred_cnn, y_pred_cnn_reg, y_pred_cnn_drop, y_pred_cnn_reg_drop]\n",
    "model_names = [\"CNN Padrão\", \"CNN com Regularização\", \"CNN com Dropout\", \"CNN com Regularização e Dropout\"]\n",
    "\n",
    "# Chamando a função para exibir os gráficos lado a lado\n",
    "plot_multiple_predictions(train_valid_data, Y_test, y_pred_models, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II9_KBOxHMvb"
   },
   "source": [
    "# LSTM for Time Series Forecasting\n",
    "\n",
    "* Now the LSTM model actually sees the input data as a sequence, so it's able to learn patterns from sequenced data (assuming it exists) better than the other ones, especially patterns from long sequences.\n",
    "* Input shape **[samples, timesteps, features]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9zJWMD-HMvc"
   },
   "source": [
    "## Pre-procesamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparametros\n",
    "epochs_lstm = 30\n",
    "batch_lstm = 32\n",
    "lr_lstm = 0.001\n",
    "neu_lstm1 = 64 #Quantidade de Neuronios da Primeira Camada\n",
    "neu_lstm2 = 64 #Quantidade de Neuronios da Segunda Camada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando Quantidade de Neuronios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a função de criação do modelo LSTM\n",
    "def create_lstm_model(neurons_layer_1, neurons_layer_2):\n",
    "    model_lstm_test = Sequential()\n",
    "    if neurons_layer_2 > 0:\n",
    "        seque = True\n",
    "    else:\n",
    "        seque = False\n",
    "    model_lstm_test.add(LSTM(neurons_layer_1, \n",
    "                             activation='tanh',\n",
    "                             return_sequences=seque, \n",
    "                             input_shape=(X_train_series.shape[1], X_train_series.shape[2])\n",
    "                             ))\n",
    "\n",
    "    # Segunda camada LSTM (apenas se neurons_layer_2 > 0)\n",
    "    if neurons_layer_2 > 0:\n",
    "        model_lstm_test.add(LSTM(neurons_layer_2, activation='tanh', return_sequences=False))\n",
    "\n",
    "    model_lstm_test.add(Dense(1))\n",
    "    model_lstm_test.compile(loss='mse', optimizer=optimizers.Adam(lr_lstm))\n",
    "    return model_lstm_test \n",
    "\n",
    "# Lista de diferentes quantidades de neurônios a testar\n",
    "neurons_layer_1_options_lstm = [16, 24, 32, 48, 64, 82, 128, 256, 512]\n",
    "neurons_layer_2_options_lstm = [0, 16, 24, 32, 48, 64, 82, 128, 256, 512]\n",
    "\n",
    "# Armazenar resultados\n",
    "results_lstm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop para testar diferentes quantidades de neurônios\n",
    "# for neurons in neurons_options:\n",
    "#     print(f\"Treinando modelo com {neurons} neurônios...\")\n",
    "    \n",
    "#     # Criar o modelo\n",
    "#     model = create_mlp_model(neurons)\n",
    "    \n",
    "#     # Treinar o modelo\n",
    "#     history = model.fit(X_train, Y_train, epochs=epochs_lstm, batch_size=batch_lstm, validation_data=(X_valid, Y_valid), verbose=0)\n",
    "    \n",
    "#     # Obter o desempenho (pode ser MAPE, RMSE, etc.)\n",
    "#     val_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "#     # Armazenar o desempenho\n",
    "#     results_lstm.append({'neurons': neurons, 'val_loss': val_loss})\n",
    "\n",
    "# # Converter para dataframe (opcional)\n",
    "# import pandas as pd\n",
    "# results_lstm_df = pd.DataFrame(results_lstm)\n",
    "\n",
    "# # Gerar gráfico\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(results_lstm_df['neurons'], results_lstm_df['val_loss'], marker='o')\n",
    "# plt.title('Desempenho por Quantidade de Neurônios')\n",
    "# plt.xlabel('Número de Neurônios')\n",
    "# plt.ylabel('Val Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop para testar todas as combinações possíveis de neurônios\n",
    "# for neurons_layer_1 in neurons_layer_1_options_lstm:\n",
    "#     for neurons_layer_2 in neurons_layer_2_options_lstm:\n",
    "#         print(f\"Treinando modelo com {neurons_layer_1} neurônios na primeira camada e {neurons_layer_2} na segunda camada...\")\n",
    "\n",
    "#         # Criar o modelo\n",
    "#         model_lstm = create_lstm_model(neurons_layer_1, neurons_layer_2)\n",
    "\n",
    "#         # Treinar o modelo\n",
    "#         history = model_lstm.fit(X_train_series, Y_train, epochs=epochs_lstm, batch_size=32, validation_data=(X_valid, Y_valid), verbose=0)\n",
    "\n",
    "#         # Obter o desempenho (pode ser MAPE, RMSE, etc.)\n",
    "#         val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "#         # Armazenar o desempenho\n",
    "#         results_lstm.append({'neurons_layer_1': neurons_layer_1, 'neurons_layer_2': neurons_layer_2, 'val_loss': val_loss})\n",
    "\n",
    "\n",
    "# results_df_lstm = pd.DataFrame(results_lstm)\n",
    "\n",
    "# # Gerar gráfico\n",
    "# fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# # Gerar gráfico de calor (heatmap)\n",
    "# pivot_table = results_df_lstm.pivot(index='neurons_layer_1', columns='neurons_layer_2', values='val_loss')\n",
    "# cax = ax.matshow(pivot_table, cmap='viridis')\n",
    "\n",
    "# # Adicionar rótulos\n",
    "# ax.set_xticks(np.arange(len(neurons_layer_2_options_lstm)))\n",
    "# ax.set_yticks(np.arange(len(neurons_layer_1_options_lstm)))\n",
    "# ax.set_xticklabels(neurons_layer_2_options_lstm)\n",
    "# ax.set_yticklabels(neurons_layer_1_options_lstm)\n",
    "\n",
    "# # Adicionar título e legendas\n",
    "# plt.xlabel('Neurônios na Segunda Camada')\n",
    "# plt.ylabel('Neurônios na Primeira Camada')\n",
    "# plt.title('Desempenho por Número de Neurônios nas Camadas Ocultas')\n",
    "# fig.colorbar(cax)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnIxi7PrHMvc"
   },
   "source": [
    "## Implementacao e Ajuste do Modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "AmDVlZgXHMvc"
   },
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(neu_lstm1, activation='relu', return_sequences=True, input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_lstm.add(LSTM(neu_lstm2, activation='relu', return_sequences=False))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mse', optimizer=optimizers.Adam(lr_lstm))\n",
    "#model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "UC6_ezAqHMvc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 209ms/step - loss: 15.1568 - val_loss: 0.0563\n",
      "Epoch 2/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 203ms/step - loss: 0.0501 - val_loss: 0.0407\n",
      "Epoch 3/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: 0.0402 - val_loss: 0.0380\n",
      "Epoch 4/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: 0.0374 - val_loss: 0.0336\n",
      "Epoch 5/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 203ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 203ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 205ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 206ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 205ms/step - loss: nan - val_loss: nan\n",
      "Epoch 21/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 22/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 204ms/step - loss: nan - val_loss: nan\n",
      "Epoch 23/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 203ms/step - loss: nan - val_loss: nan\n",
      "Epoch 24/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 199ms/step - loss: nan - val_loss: nan\n",
      "Epoch 25/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 199ms/step - loss: nan - val_loss: nan\n",
      "Epoch 26/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 199ms/step - loss: nan - val_loss: nan\n",
      "Epoch 27/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 198ms/step - loss: nan - val_loss: nan\n",
      "Epoch 28/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 198ms/step - loss: nan - val_loss: nan\n",
      "Epoch 29/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 198ms/step - loss: nan - val_loss: nan\n",
      "Epoch 30/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 198ms/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(X_train_series,\n",
    "                              Y_train,\n",
    "                              validation_data=(X_valid, Y_valid),\n",
    "                              epochs=epochs_lstm,\n",
    "                              batch_size = batch_lstm,\n",
    "                                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3uUQ49MHMve"
   },
   "source": [
    "## Implementacao e Ajuste do Modelo LSTM com Regularizacao L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "8MjGfIOpHMve"
   },
   "outputs": [],
   "source": [
    "model_lstm_reg = Sequential()\n",
    "model_lstm_reg.add(LSTM(neu_lstm1, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_lstm_reg.add(Dense(1))\n",
    "model_lstm_reg.compile(optimizer=optimizers.Adam(lr_lstm), loss='mse')\n",
    "#model_lstm_reg.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "dIgksc-GHMvf"
   },
   "outputs": [],
   "source": [
    "history_lstm_reg = model_lstm_reg.fit(X_train_series,\n",
    "                              Y_train,\n",
    "                              validation_data=(X_valid, Y_valid),\n",
    "                              epochs=epochs_lstm,\n",
    "                              batch_size = batch_lstm,\n",
    "                                verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWC9oKAuHMvf"
   },
   "source": [
    "## Implementacao e Ajuste do Modelo LSTM com Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "pC2W0gPwHMvf"
   },
   "outputs": [],
   "source": [
    "model_lstm_drop = Sequential()\n",
    "model_lstm_drop.add(LSTM(neu_lstm1, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n",
    "model_lstm_drop.add(Dropout(droprate))\n",
    "model_lstm_drop.add(Dense(1))\n",
    "model_lstm_drop.compile(optimizer=optimizers.Adam(lr_lstm), loss='mse')\n",
    "#model_lstm_dropout.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "eZmgaT7tHMvg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 85ms/step - loss: 0.1235 - val_loss: 0.0234\n",
      "Epoch 2/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0291 - val_loss: 0.0119\n",
      "Epoch 3/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0196 - val_loss: 0.0088\n",
      "Epoch 4/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0159 - val_loss: 0.0071\n",
      "Epoch 5/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0135 - val_loss: 0.0069\n",
      "Epoch 6/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0128 - val_loss: 0.0065\n",
      "Epoch 7/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0119 - val_loss: 0.0051\n",
      "Epoch 8/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0100 - val_loss: 0.0042\n",
      "Epoch 9/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0095 - val_loss: 0.0041\n",
      "Epoch 10/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0087 - val_loss: 0.0043\n",
      "Epoch 11/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0075 - val_loss: 0.0048\n",
      "Epoch 12/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0081 - val_loss: 0.0033\n",
      "Epoch 13/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0067 - val_loss: 0.0031\n",
      "Epoch 14/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0065 - val_loss: 0.0034\n",
      "Epoch 15/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0060 - val_loss: 0.0030\n",
      "Epoch 16/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0050 - val_loss: 0.0024\n",
      "Epoch 17/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0047 - val_loss: 0.0032\n",
      "Epoch 18/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0046 - val_loss: 0.0020\n",
      "Epoch 19/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 83ms/step - loss: 0.0038 - val_loss: 0.0023\n",
      "Epoch 20/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0039 - val_loss: 0.0025\n",
      "Epoch 21/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0036 - val_loss: 0.0027\n",
      "Epoch 22/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0036 - val_loss: 0.0025\n",
      "Epoch 23/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - loss: 0.0036 - val_loss: 0.0021\n",
      "Epoch 24/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0033 - val_loss: 0.0026\n",
      "Epoch 25/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0034 - val_loss: 0.0023\n",
      "Epoch 26/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0033 - val_loss: 0.0028\n",
      "Epoch 27/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0032 - val_loss: 0.0026\n",
      "Epoch 28/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 29/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 84ms/step - loss: 0.0031 - val_loss: 0.0021\n",
      "Epoch 30/30\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - loss: 0.0029 - val_loss: 0.0022\n"
     ]
    }
   ],
   "source": [
    "# Treinamento do modelo com Dropout\n",
    "history_lstm_drop = model_lstm_drop.fit(X_train_series,\n",
    "                                              Y_train,\n",
    "                                              epochs=epochs_lstm,\n",
    "                                              batch_size=batch_lstm,\n",
    "                                              validation_data=(X_valid, Y_valid),\n",
    "                                              verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EwgWpJGHMvg"
   },
   "source": [
    "## Implementacao e Ajuste do Modelo LSTM com Regularizacao e Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "iyEwnRTAHMvg"
   },
   "outputs": [],
   "source": [
    "model_lstm_reg_drop = Sequential()\n",
    "model_lstm_reg_drop.add(\n",
    "    LSTM(\n",
    "        neu_lstm1,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(0.01),  # Regularização L2\n",
    "        input_shape=(X_train_series.shape[1], X_train_series.shape[2])\n",
    "    )\n",
    ")\n",
    "model_lstm_reg_drop.add(Dropout(droprate))\n",
    "model_lstm_reg_drop.add(Dense(1))\n",
    "model_lstm_reg_drop.compile(loss='mse', optimizer=optimizers.Adam(lr_lstm))\n",
    "#model_lstm_reg_dropout.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "vTkCCG1MHMvg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "126/126 - 12s - 96ms/step - loss: 0.0655 - val_loss: 0.0334\n",
      "Epoch 2/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0329 - val_loss: 0.0173\n",
      "Epoch 3/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0232 - val_loss: 0.0143\n",
      "Epoch 4/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0202 - val_loss: 0.0129\n",
      "Epoch 5/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0182 - val_loss: 0.0112\n",
      "Epoch 6/30\n",
      "126/126 - 10s - 81ms/step - loss: 0.0168 - val_loss: 0.0124\n",
      "Epoch 7/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0154 - val_loss: 0.0092\n",
      "Epoch 8/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0136 - val_loss: 0.0086\n",
      "Epoch 9/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0127 - val_loss: 0.0087\n",
      "Epoch 10/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0125 - val_loss: 0.0077\n",
      "Epoch 11/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0113 - val_loss: 0.0080\n",
      "Epoch 12/30\n",
      "126/126 - 10s - 81ms/step - loss: 0.0109 - val_loss: 0.0068\n",
      "Epoch 13/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0101 - val_loss: 0.0069\n",
      "Epoch 14/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0097 - val_loss: 0.0072\n",
      "Epoch 15/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0094 - val_loss: 0.0063\n",
      "Epoch 16/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0084 - val_loss: 0.0060\n",
      "Epoch 17/30\n",
      "126/126 - 10s - 81ms/step - loss: 0.0079 - val_loss: 0.0053\n",
      "Epoch 18/30\n",
      "126/126 - 10s - 81ms/step - loss: 0.0078 - val_loss: 0.0052\n",
      "Epoch 19/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0072 - val_loss: 0.0048\n",
      "Epoch 20/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0069 - val_loss: 0.0064\n",
      "Epoch 21/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0066 - val_loss: 0.0051\n",
      "Epoch 22/30\n",
      "126/126 - 11s - 90ms/step - loss: 0.0064 - val_loss: 0.0049\n",
      "Epoch 23/30\n",
      "126/126 - 12s - 93ms/step - loss: 0.0064 - val_loss: 0.0044\n",
      "Epoch 24/30\n",
      "126/126 - 12s - 93ms/step - loss: 0.0062 - val_loss: 0.0050\n",
      "Epoch 25/30\n",
      "126/126 - 11s - 89ms/step - loss: 0.0060 - val_loss: 0.0043\n",
      "Epoch 26/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0058 - val_loss: 0.0045\n",
      "Epoch 27/30\n",
      "126/126 - 10s - 81ms/step - loss: 0.0059 - val_loss: 0.0057\n",
      "Epoch 28/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0057 - val_loss: 0.0048\n",
      "Epoch 29/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 30/30\n",
      "126/126 - 10s - 80ms/step - loss: 0.0054 - val_loss: 0.0043\n"
     ]
    }
   ],
   "source": [
    "history_lstm_reg_drop = model_lstm_reg_drop.fit(X_train_series,\n",
    "                            Y_train,\n",
    "                            epochs=epochs_lstm,\n",
    "                            batch_size=batch_lstm,\n",
    "                            validation_data=(X_valid, Y_valid),\n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFPjLLGKHMvh"
   },
   "source": [
    "## Comparativo entre os modelos - Predicao e Validacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "cIeRijmCHMvh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Avaliação de cada modelo no conjunto de teste\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mae_standard, mse_standard, rmse_standard, mape_standard \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModelo Padrão\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m mae_reg, mse_reg, rmse_reg, mape_reg \u001b[38;5;241m=\u001b[39m evaluate_model(model_lstm_reg, X_test, Y_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelo com Regularização\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m mae_dropout, mse_dropout, rmse_dropout, mape_dropout \u001b[38;5;241m=\u001b[39m evaluate_model(model_lstm_drop, X_test, Y_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelo com Dropout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, X_test, Y_test, model_name)\u001b[0m\n\u001b[0;32m      4\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calcula as métricas\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m mae \u001b[38;5;241m=\u001b[39m \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(Y_test, Y_pred)\n\u001b[0;32m      9\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mse)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:216\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    153\u001b[0m     {\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m ):\n\u001b[0;32m    164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    0.85...\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    220\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(np\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:113\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m    112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m--> 113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    116\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(y_true, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# Avaliação de cada modelo no conjunto de teste\n",
    "mae_standard, mse_standard, rmse_standard, mape_standard = evaluate_model(model_lstm, X_test, Y_test, \"Modelo Padrão\")\n",
    "mae_reg, mse_reg, rmse_reg, mape_reg = evaluate_model(model_lstm_reg, X_test, Y_test, \"Modelo com Regularização\")\n",
    "mae_dropout, mse_dropout, rmse_dropout, mape_dropout = evaluate_model(model_lstm_drop, X_test, Y_test, \"Modelo com Dropout\")\n",
    "mae_reg_drop, mse_reg_drop, rmse_reg_drop, mape_reg_drop = evaluate_model(model_lstm_reg_drop, X_test, Y_test, \"Modelo com Regularização e Dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JfIVNNZHMvh"
   },
   "outputs": [],
   "source": [
    "#Plot do Grafico de Perda\n",
    "plot_multiple_loss(history_lstm, history_lstm_reg, history_lstm_drop, history_lstm_reg_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svLONEFZHMvh"
   },
   "outputs": [],
   "source": [
    "# Obtendo as previsões dos quatro modelos\n",
    "y_pred_lstm = model_lstm.predict(X_test)\n",
    "y_pred_lstm_reg = model_lstm_reg.predict(X_test)\n",
    "y_pred_lstm_drop = model_lstm_drop.predict(X_test)\n",
    "y_pred_lstm_reg_drop = model_lstm_reg_drop.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iP_gUrInHMvh"
   },
   "outputs": [],
   "source": [
    "plot_predicts(y_pred_lstm, y_pred_lstm_reg, y_pred_lstm_drop, y_pred_lstm_reg_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikjvUu5nHMvi"
   },
   "outputs": [],
   "source": [
    "# Combine os dados de treino e validação em uma série para comparação visual com o conjunto de teste e previsões.\n",
    "train_valid_data = np.concatenate([Y_train, Y_valid])  # Combina Y_train e Y_valid em uma série contínua\n",
    "\n",
    "# Lista com previsões e nomes dos modelos para plotar\n",
    "y_pred_models = [y_pred_lstm, y_pred_lstm_reg, y_pred_lstm_drop, y_pred_lstm_reg_drop]\n",
    "model_names = [\"LSTM Padrão\", \"LSTM com Regularização\", \"LSTM com Dropout\", \"LSTM com Regularização e Dropout\"]\n",
    "\n",
    "# Chamando a função para exibir os gráficos lado a lado\n",
    "plot_multiple_predictions(train_valid_data, Y_test, y_pred_models, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvyVqHqgHMvi"
   },
   "source": [
    "# CNN-LSTM for Time Series Forecasting\n",
    "* Input shape **[samples, subsequences, timesteps, features]**.\n",
    "\n",
    "#### Model explanation from the [article](https://machinelearningmastery.com/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course/)\n",
    "> \"The benefit of this model is that the model can support very long input sequences that can be read as blocks or subsequences by the CNN model, then pieced together by the LSTM model.\"\n",
    ">\n",
    "> \"When using a hybrid CNN-LSTM model, we will further divide each sample into further subsequences. The CNN model will interpret each sub-sequence and the LSTM will piece together the interpretations from the subsequences. As such, we will split each sample into 2 subsequences of 2 times per subsequence.\"\n",
    ">\n",
    "> \"The CNN will be defined to expect 2 timesteps per subsequence with one feature. The entire CNN model is then wrapped in TimeDistributed wrapper layers so that it can be applied to each subsequence in the sample. The results are then interpreted by the LSTM layer before the model outputs a prediction.\"\n",
    "\n",
    "#### Data preprocess\n",
    "* Reshape from [samples, timesteps, features] into [samples, subsequences, timesteps, features]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS7ff_6_HMvi"
   },
   "source": [
    "## Pre-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dBeTanfHMvi"
   },
   "outputs": [],
   "source": [
    "#Hiperparametros\n",
    "epochs_hibrid = 30\n",
    "batch_hibrid = 32\n",
    "lr_hibrid = 0.001\n",
    "subsequences = 2\n",
    "neu_cnn = 256 #Quantidade de Neuronios na Camada de CNN\n",
    "neu_lstm = 64 #Quantidade de Neuronios na Camada de LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWo_lc-ZHMvi"
   },
   "outputs": [],
   "source": [
    "timesteps = X_train.shape[1]//subsequences\n",
    "X_train_series_sub = X_train.reshape((X_train.shape[0], subsequences, timesteps, 1))\n",
    "X_valid_series_sub = X_valid.reshape((X_valid.shape[0], subsequences, timesteps, 1))\n",
    "X_test_series_sub = X_test.reshape((X_test.shape[0], subsequences, timesteps, 1))\n",
    "print('Train set shape', X_train_series_sub.shape)\n",
    "print('Validation set shape', X_valid_series_sub.shape)\n",
    "print('Test set shape', X_test_series_sub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando a Quantidade de Neuronios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(neurons_layer_1, neurons_layer_2):\n",
    "    model_cnn_lstm = Sequential()\n",
    "    model_cnn_lstm.add(TimeDistributed(Conv1D(filters=neurons_layer_1, kernel_size=1, activation='relu'), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "    model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "    model_cnn_lstm.add(LSTM(neurons_layer_2, activation='relu'))\n",
    "    model_cnn_lstm.add(Dense(1))\n",
    "    model_cnn_lstm.compile(loss='mse', optimizer=optimizers.Adam(lr_hibrid))\n",
    "    return model\n",
    "\n",
    "\n",
    "# Listas de neurônios para a primeira e segunda camada\n",
    "neurons_layer_1_options_cnn_lstm = [32, 64, 128, 256]\n",
    "neurons_layer_2_options_cnn_lstm = [32, 64, 128, 256]\n",
    "\n",
    "# Armazenar resultados\n",
    "results_cnn_lstm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop para testar todas as combinações possíveis de neurônios\n",
    "for neurons_layer_1 in neurons_layer_1_options_cnn_lstm:\n",
    "    for neurons_layer_2 in neurons_layer_2_options_cnn_lstm:\n",
    "        print(f\"Treinando modelo com {neurons_layer_1} neurônios na primeira camada e {neurons_layer_2} na segunda camada...\")\n",
    "\n",
    "        # Criar o modelo\n",
    "        model = create_mlp_model(neurons_layer_1, neurons_layer_2)\n",
    "\n",
    "        # Treinar o modelo\n",
    "        history = model.fit(X_train, Y_train, epochs=epochs_hibrid, batch_size=batch_hibrid, validation_data=(X_valid, Y_valid), verbose=0)\n",
    "\n",
    "        # Obter o desempenho (pode ser MAPE, RMSE, etc.)\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "        # Armazenar o desempenho\n",
    "        results_cnn_lstm.append({'neurons_layer_1': neurons_layer_1, 'neurons_layer_2': neurons_layer_2, 'val_loss': val_loss})\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results_cnn_lstm)\n",
    "\n",
    "# Gerar gráfico\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Gerar gráfico de calor (heatmap)\n",
    "pivot_table = results_df.pivot(index='neurons_layer_1', columns='neurons_layer_2', values='val_loss')\n",
    "cax = ax.matshow(pivot_table, cmap='viridis')\n",
    "\n",
    "# Adicionar rótulos\n",
    "ax.set_xticks(np.arange(len(neurons_layer_2_options_cnn_lstm)))\n",
    "ax.set_yticks(np.arange(len(neurons_layer_1_options_cnn_lstm)))\n",
    "ax.set_xticklabels(neurons_layer_2_options_cnn_lstm)\n",
    "ax.set_yticklabels(neurons_layer_1_options_cnn_lstm)\n",
    "\n",
    "# Adicionar título e legendas\n",
    "plt.xlabel('Neurônios na Segunda Camada')\n",
    "plt.ylabel('Neurônios na Primeira Camada')\n",
    "plt.title('Desempenho por Número de Neurônios nas Camadas Ocultas')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MitYALM9HMvi"
   },
   "source": [
    "## Implementacao e ajuste do modelo padrao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YYSmCmvHMvj"
   },
   "outputs": [],
   "source": [
    "model_cnn_lstm = Sequential()\n",
    "model_cnn_lstm.add(TimeDistributed(Conv1D(filters=neu_cnn, kernel_size=1, activation='relu'), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "model_cnn_lstm.add(LSTM(neu_lstm, activation='relu'))\n",
    "model_cnn_lstm.add(Dense(1))\n",
    "model_cnn_lstm.compile(loss='mse', optimizer=optimizers.Adam(lr_hibrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tT7KggyHMvj"
   },
   "outputs": [],
   "source": [
    "history_cnn_lstm = model_cnn_lstm.fit(X_train_series_sub,\n",
    "                                      Y_train,\n",
    "                                      validation_data=(X_valid_series_sub, Y_valid),\n",
    "                                      epochs=epochs_hibrid,\n",
    "                                      batch_size = batch_hibrid,\n",
    "                                      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "je-YY38HHMvj"
   },
   "source": [
    "## Implementacao e Ajuste do modelo com Regularizacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HkCPkc3HMvj"
   },
   "outputs": [],
   "source": [
    "model_cnn_lstm_reg = Sequential()\n",
    "model_cnn_lstm_reg.add(TimeDistributed(Conv1D(filters=neu_cnn, kernel_size=1, activation='relu', kernel_regularizer=regularizers.l2(0.001)), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "model_cnn_lstm_reg.add(TimeDistributed(MaxPooling1D(pool_size=1)))\n",
    "model_cnn_lstm_reg.add(TimeDistributed(Flatten()))\n",
    "model_cnn_lstm_reg.add(LSTM(neu_lstm, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model_cnn_lstm_reg.add(Dense(1))\n",
    "model_cnn_lstm_reg.compile(loss='mse', optimizer=optimizers.Adam(lr_hibrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kjwb9j56HMvj"
   },
   "outputs": [],
   "source": [
    "history_cnn_lstm_reg = model_cnn_lstm_reg.fit(X_train_series_sub,\n",
    "                                              Y_train,\n",
    "                                              validation_data=(X_valid_series_sub, Y_valid),\n",
    "                                              epochs=epochs_hibrid,\n",
    "                                              batch_size = batch_hibrid,\n",
    "                                              verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tIGfnMXHMvj"
   },
   "source": [
    "## Implementacao e Ajuste do Modelo com Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5h5xmqLHMvj"
   },
   "outputs": [],
   "source": [
    "model_cnn_lstm_drop = Sequential()\n",
    "model_cnn_lstm_drop.add(TimeDistributed(Conv1D(filters=neu_cnn, kernel_size=1, activation='relu'), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "model_cnn_lstm_drop.add(TimeDistributed(MaxPooling1D(pool_size=1)))\n",
    "model_cnn_lstm_drop.add(TimeDistributed(Flatten()))\n",
    "model_cnn_lstm_drop.add(Dropout(droprate))\n",
    "model_cnn_lstm_drop.add(LSTM(neu_lstm, activation='relu'))\n",
    "model_cnn_lstm_drop.add(Dropout(droprate))\n",
    "model_cnn_lstm_drop.add(Dense(1))\n",
    "model_cnn_lstm_drop.compile(loss='mse', optimizer=optimizers.Adam(lr_hibrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "De8jb85xHMvk"
   },
   "outputs": [],
   "source": [
    "history_cnn_lstm_drop = model_cnn_lstm_drop.fit(X_train_series_sub,\n",
    "                                                Y_train,\n",
    "                                                validation_data=(X_valid_series_sub, Y_valid),\n",
    "                                                epochs=epochs_hibrid,\n",
    "                                                batch_size = batch_hibrid,\n",
    "                                                verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6rnbQt3HMvk"
   },
   "source": [
    "## Implementacao e Ajuste do Modelo com Regularizacao e Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMVisWLkHMvk"
   },
   "outputs": [],
   "source": [
    "model_cnn_lstm_reg_drop = Sequential()\n",
    "model_cnn_lstm_reg_drop.add(TimeDistributed(Conv1D(filters=neu_cnn, kernel_size=1, activation='relu', kernel_regularizer=regularizers.l2(0.001)), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\n",
    "model_cnn_lstm_reg_drop.add(TimeDistributed(MaxPooling1D(pool_size=1)))\n",
    "model_cnn_lstm_reg_drop.add(TimeDistributed(Flatten()))\n",
    "model_cnn_lstm_reg_drop.add(Dropout(droprate))\n",
    "model_cnn_lstm_reg_drop.add(LSTM(neu_lstm, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model_cnn_lstm_reg_drop.add(Dropout(droprate))\n",
    "model_cnn_lstm_reg_drop.add(Dense(1))\n",
    "model_cnn_lstm_reg_drop.compile(loss='mse', optimizer=optimizers.Adam(lr_hibrid))\n",
    "\n",
    "history_cnn_lstm_reg_drop = model_cnn_lstm_reg_drop.fit(X_train_series_sub,\n",
    "                                                        Y_train,\n",
    "                                                        validation_data=(X_valid_series_sub, Y_valid),\n",
    "                                                        epochs=epochs_hibrid,\n",
    "                                                        batch_size = batch_hibrid,\n",
    "                                                        verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GtRCoymHMvl"
   },
   "source": [
    "## Comparativo entre os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoOWIM-BHMvl"
   },
   "outputs": [],
   "source": [
    "# Realizando previsões para cada modelo\n",
    "y_pred_cnn_lstm = model_cnn_lstm.predict(X_test_series_sub)\n",
    "y_pred_cnn_lstm_reg = model_cnn_lstm_reg.predict(X_test_series_sub)\n",
    "y_pred_cnn_lstm_drop = model_cnn_lstm_drop.predict(X_test_series_sub)\n",
    "y_pred_cnn_lstm_reg_drop = model_cnn_lstm_reg_drop.predict(X_test_series_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSWoZbRlHMvm"
   },
   "outputs": [],
   "source": [
    "# Avaliação de cada modelo no conjunto de teste\n",
    "mae_standard, mse_standard, rmse_standard, mape_standard = evaluate_model(model_cnn_lstm, X_test_series_sub, Y_test, \"Modelo Padrão\")\n",
    "mae_reg, mse_reg, rmse_reg, mape_reg = evaluate_model(model_cnn_lstm_reg, X_test_series_sub, Y_test, \"Modelo com Regularização\")\n",
    "mae_dropout, mse_dropout, rmse_dropout, mape_dropout = evaluate_model(model_cnn_lstm_drop, X_test_series_sub, Y_test, \"Modelo com Dropout\")\n",
    "mae_reg_drop, mse_reg_drop, rmse_reg_drop, mape_reg_drop = evaluate_model(model_cnn_lstm_reg_drop, X_test_series_sub, Y_test, \"Modelo com Regularização e Dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oao_IqTTHMvm"
   },
   "outputs": [],
   "source": [
    "#Plot do Grafico de Perda\n",
    "plot_multiple_loss(history_cnn_lstm, history_cnn_lstm_reg, history_cnn_lstm_drop, history_cnn_lstm_reg_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziLlks0pHMvm"
   },
   "outputs": [],
   "source": [
    "plot_predicts(y_pred_cnn_lstm, y_pred_cnn_lstm_reg, y_pred_cnn_lstm_drop, y_pred_cnn_lstm_reg_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ElnXLTpHMvm"
   },
   "outputs": [],
   "source": [
    "# Combine os dados de treino e validação em uma série para comparação visual com o conjunto de teste e previsões.\n",
    "train_valid_data = np.concatenate([Y_train, Y_valid])  # Combina Y_train e Y_valid em uma série contínua\n",
    "\n",
    "# Lista com previsões e nomes dos modelos para plotar\n",
    "y_pred_models = [y_pred_cnn_lstm, y_pred_cnn_lstm_reg, y_pred_cnn_lstm_drop, y_pred_cnn_lstm_reg_drop]\n",
    "model_names = [\"CNN-LSTM Padrão\", \"CNN-LSTM com Regularização\", \"CNN-LSTM com Dropout\", \"CNN-LSTM com Regularização e Dropout\"]\n",
    "\n",
    "# Chamando a função para exibir os gráficos lado a lado\n",
    "plot_multiple_predictions(train_valid_data, Y_test, y_pred_models, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando Looping de Neuronios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir a função de criação do modelo MLP (como exemplo)\n",
    "def create_mlp_model(neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=X_train.shape[1]))\n",
    "    model.add(Dense(1))  # Camada de saída para regressão\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Lista de diferentes quantidades de neurônios a testar\n",
    "neurons_options = [16, 24, 32, 48, 64, 82, 128, 256, 512]\n",
    "\n",
    "# Armazenar resultados\n",
    "results = []\n",
    "\n",
    "# Loop para testar diferentes quantidades de neurônios\n",
    "for neurons in neurons_options:\n",
    "    print(f\"Treinando modelo com {neurons} neurônios...\")\n",
    "    \n",
    "    # Criar o modelo\n",
    "    model = create_mlp_model(neurons)\n",
    "    \n",
    "    # Treinar o modelo\n",
    "    history = model.fit(X_train, Y_train, epochs=50, batch_size=32, validation_data=(X_valid, Y_valid), verbose=0)\n",
    "    \n",
    "    # Obter o desempenho (pode ser MAPE, RMSE, etc.)\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    # Armazenar o desempenho\n",
    "    results.append({'neurons': neurons, 'val_loss': val_loss})\n",
    "\n",
    "# Converter para dataframe (opcional)\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Gerar gráfico\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(results_df['neurons'], results_df['val_loss'], marker='o')\n",
    "plt.title('Desempenho por Quantidade de Neurônios')\n",
    "plt.xlabel('Número de Neurônios')\n",
    "plt.ylabel('Val Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Função de criação do modelo MLP com 2 camadas ocultas\n",
    "def create_mlp_model(neurons_layer_1, neurons_layer_2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons_layer_1, activation='relu', input_dim=X_train.shape[1]))\n",
    "    model.add(Dense(neurons_layer_2, activation='relu'))  # Segunda camada\n",
    "    model.add(Dense(1))  # Camada de saída para regressão\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Listas de neurônios para a primeira e segunda camada\n",
    "neurons_layer_1_options = [32, 64, 128, 256]\n",
    "neurons_layer_2_options = [32, 64, 128, 256]\n",
    "\n",
    "# Armazenar resultados\n",
    "results = []\n",
    "\n",
    "# Loop para testar todas as combinações possíveis de neurônios\n",
    "for neurons_layer_1 in neurons_layer_1_options:\n",
    "    for neurons_layer_2 in neurons_layer_2_options:\n",
    "        print(f\"Treinando modelo com {neurons_layer_1} neurônios na primeira camada e {neurons_layer_2} na segunda camada...\")\n",
    "\n",
    "        # Criar o modelo\n",
    "        model = create_mlp_model(neurons_layer_1, neurons_layer_2)\n",
    "\n",
    "        # Treinar o modelo\n",
    "        history = model.fit(X_train, Y_train, epochs=35, batch_size=32, validation_data=(X_valid, Y_valid), verbose=0)\n",
    "\n",
    "        # Obter o desempenho (pode ser MAPE, RMSE, etc.)\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "        # Armazenar o desempenho\n",
    "        results.append({'neurons_layer_1': neurons_layer_1, 'neurons_layer_2': neurons_layer_2, 'val_loss': val_loss})\n",
    "\n",
    "# Converter para dataframe (opcional)\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Gerar gráfico\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Gerar gráfico de calor (heatmap)\n",
    "pivot_table = results_df.pivot(index='neurons_layer_1', columns='neurons_layer_2', values='val_loss')\n",
    "cax = ax.matshow(pivot_table, cmap='viridis')\n",
    "\n",
    "# Adicionar rótulos\n",
    "ax.set_xticks(np.arange(len(neurons_layer_2_options)))\n",
    "ax.set_yticks(np.arange(len(neurons_layer_1_options)))\n",
    "ax.set_xticklabels(neurons_layer_2_options)\n",
    "ax.set_yticklabels(neurons_layer_1_options)\n",
    "\n",
    "# Adicionar título e legendas\n",
    "plt.xlabel('Neurônios na Segunda Camada')\n",
    "plt.ylabel('Neurônios na Primeira Camada')\n",
    "plt.title('Desempenho por Número de Neurônios nas Camadas Ocultas')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
